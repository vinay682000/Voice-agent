<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Assistant (WebRTC)</title>
    <style>
        :root {
            --primary: #6366f1;
            --primary-dark: #4f46e5;
            --bg: #0f172a;
            --card: #1e293b;
            --text: #f1f5f9;
            --text-muted: #94a3b8;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: var(--bg);
            color: var(--text);
            min-height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
        }

        .container {
            background: var(--card);
            padding: 2.5rem;
            border-radius: 1.5rem;
            box-shadow: 0 25px 50px -12px rgba(0, 0, 0, 0.5);
            text-align: center;
            max-width: 450px;
            width: 90%;
        }

        h1 {
            font-size: 1.75rem;
            margin-bottom: 0.5rem;
            background: linear-gradient(135deg, var(--primary), #a855f7);
            background-clip: text;
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        .status {
            color: var(--text-muted);
            margin-bottom: 1.5rem;
            font-size: 0.9rem;
            min-height: 1.2rem;
        }

        .visualizer {
            display: flex;
            justify-content: center;
            gap: 6px;
            height: 60px;
            align-items: flex-end;
            margin-bottom: 1.5rem;
        }

        .bar {
            width: 8px;
            background: linear-gradient(to top, var(--primary), #a855f7);
            border-radius: 4px;
            transition: height 0.1s ease;
            height: 10px;
        }

        .bar.active {
            animation: pulse 0.5s ease-in-out infinite alternate;
        }

        @keyframes pulse {
            from {
                height: 10px;
                opacity: 0.6;
            }

            to {
                height: 50px;
                opacity: 1;
            }
        }

        button {
            background: linear-gradient(135deg, var(--primary), var(--primary-dark));
            color: white;
            border: none;
            padding: 1rem 2.5rem;
            font-size: 1.1rem;
            border-radius: 50px;
            cursor: pointer;
            transition: all 0.3s ease;
            font-weight: 600;
        }

        button:hover {
            transform: translateY(-2px);
            box-shadow: 0 10px 20px rgba(99, 102, 241, 0.3);
        }

        button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
        }

        .transcript {
            margin-top: 1.5rem;
            padding: 1rem;
            background: rgba(0, 0, 0, 0.2);
            border-radius: 0.75rem;
            max-height: 200px;
            overflow-y: auto;
            text-align: left;
            font-size: 0.85rem;
            line-height: 1.6;
        }

        .transcript .user {
            color: #60a5fa;
        }

        .transcript .agent {
            color: #34d399;
        }
    </style>
</head>

<body>
    <div class="container">
        <h1>Voice Assistant</h1>
        <div id="status" class="status">Click below to start</div>
        <div class="visualizer" id="visualizer">
            <div class="bar"></div>
            <div class="bar"></div>
            <div class="bar"></div>
            <div class="bar"></div>
            <div class="bar"></div>
            <div class="bar"></div>
            <div class="bar"></div>
            <div class="bar"></div>
        </div>
        <button id="startBtn">Start Conversation</button>
        <div id="transcript" class="transcript"><i>Transcript will appear here...</i></div>
    </div>

    <script>
        const statusEl = document.getElementById('status');
        const startBtn = document.getElementById('startBtn');
        const transcriptEl = document.getElementById('transcript');
        const bars = document.querySelectorAll('.bar');

        let peerConnection = null;
        let dataChannel = null;
        let audioElement = null;

        startBtn.onclick = async () => {
            if (peerConnection) {
                // Stop the session
                peerConnection.close();
                peerConnection = null;
                dataChannel = null;
                startBtn.textContent = 'Start Conversation';
                statusEl.innerText = 'Session ended.';
                bars.forEach(b => b.classList.remove('active'));
                return;
            }

            startBtn.disabled = true;
            statusEl.innerText = "Getting Azure config...";

            try {
                // 1. Get Azure config (including API key) from our backend
                const configRes = await fetch('/config');
                const config = await configRes.json();

                // 2. Get ephemeral token from our backend
                statusEl.innerText = "Fetching session token...";
                const tokenRes = await fetch('/session');
                const tokenData = await tokenRes.json();

                if (tokenData.error) {
                    throw new Error(tokenData.error);
                }

                const ephemeralToken = tokenData.token;

                statusEl.innerText = "Connecting to Azure...";

                // 2. Create RTCPeerConnection
                peerConnection = new RTCPeerConnection();

                // 3. Set up audio element to play remote audio
                audioElement = document.createElement('audio');
                audioElement.autoplay = true;
                peerConnection.ontrack = (event) => {
                    audioElement.srcObject = event.streams[0];
                    bars.forEach(b => b.classList.add('active'));
                };

                // 4. Get user microphone and add track
                const stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true
                    }
                });
                stream.getTracks().forEach(track => peerConnection.addTrack(track, stream));

                // 5. Create data channel for messages
                dataChannel = peerConnection.createDataChannel('oai-events');
                dataChannel.onopen = () => {
                    console.log('Data channel open');
                    statusEl.innerText = "Connected! Start speaking...";
                };
                dataChannel.onmessage = handleDataChannelMessage;

                // 6. Create offer
                const offer = await peerConnection.createOffer();
                await peerConnection.setLocalDescription(offer);

                // 7. Send offer to Azure OpenAI Realtime API (per Microsoft docs: no query params)
                const sdpAnswerUrl = `https://${config.hostname}/openai/v1/realtime/calls`;

                const sdpResponse = await fetch(sdpAnswerUrl, {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${ephemeralToken}`,
                        'Content-Type': 'application/sdp'
                    },
                    body: offer.sdp
                });

                if (!sdpResponse.ok) {
                    const errorText = await sdpResponse.text();
                    console.error('Azure SDP exchange failed:', errorText);
                    throw new Error(`SDP exchange failed: ${sdpResponse.status}`);
                }

                const answerSdp = await sdpResponse.text();
                await peerConnection.setRemoteDescription({
                    type: 'answer',
                    sdp: answerSdp
                });

                startBtn.textContent = 'End Conversation';
                startBtn.disabled = false;
                transcriptEl.innerHTML = '';

            } catch (error) {
                console.error('Connection error:', error);
                statusEl.innerText = `Error: ${error.message}`;
                startBtn.disabled = false;
                if (peerConnection) {
                    peerConnection.close();
                    peerConnection = null;
                }
            }
        };

        function handleDataChannelMessage(event) {
            const msg = JSON.parse(event.data);
            console.log('Azure Event:', msg.type, msg);

            if (msg.type === 'conversation.item.input_audio_transcription.completed') {
                addToTranscript('user', msg.transcript);
            } else if (msg.type === 'response.audio_transcript.done') {
                addToTranscript('agent', msg.transcript);
            } else if (msg.type === 'response.function_call_arguments.done') {
                // Tool call - hit our backend
                handleToolCall(msg);
            } else if (msg.type === 'session.created') {
                console.log('Session created:', msg.session);
            } else if (msg.type === 'error') {
                console.error('Azure Error:', msg.error);
                statusEl.innerText = `Error: ${msg.error.message}`;
            }
        }

        async function handleToolCall(msg) {
            if (msg.name === 'search_knowledge_base') {
                const args = JSON.parse(msg.arguments);
                console.log('Tool Call: search_knowledge_base', args.query);

                try {
                    const response = await fetch('/search_kb', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({ query: args.query })
                    });
                    const data = await response.json();

                    // Send tool result back to Azure
                    if (dataChannel && dataChannel.readyState === 'open') {
                        dataChannel.send(JSON.stringify({
                            type: 'conversation.item.create',
                            item: {
                                type: 'function_call_output',
                                call_id: msg.call_id,
                                output: data.result
                            }
                        }));
                        dataChannel.send(JSON.stringify({ type: 'response.create' }));
                    }
                } catch (e) {
                    console.error('Tool call error:', e);
                }
            }
        }

        function addToTranscript(role, text) {
            if (!text || text.trim() === '') return;
            const div = document.createElement('div');
            div.className = role;
            div.innerHTML = `<strong>${role === 'user' ? 'You' : 'Agent'}:</strong> ${text}`;
            transcriptEl.appendChild(div);
            transcriptEl.scrollTop = transcriptEl.scrollHeight;
        }
    </script>
</body>

</html>